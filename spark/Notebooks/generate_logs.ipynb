{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "832ecdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/22 18:58:46 WARN SparkContext: The path /root/airflowmount/spark/dependencies/clfparser.zip has been added already. Overwriting of added paths is not supported in the current version.\n",
      "22/01/22 18:58:46 WARN SparkContext: The path /root/airflowmount/spark/dependencies/faker.zip has been added already. Overwriting of added paths is not supported in the current version.\n",
      "22/01/22 18:58:46 WARN SparkContext: The path /root/airflowmount/spark/dependencies/text_unidecode.zip has been added already. Overwriting of added paths is not supported in the current version.\n",
      "22/01/22 18:58:46 WARN SparkContext: The path /root/airflowmount/spark/dependencies/six.py has been added already. Overwriting of added paths is not supported in the current version.\n",
      "22/01/22 18:58:46 WARN SparkContext: The path /root/airflowmount/spark/dependencies/random_user_agent.zip has been added already. Overwriting of added paths is not supported in the current version.\n",
      "22/01/22 18:58:46 WARN SparkContext: The path /root/airflowmount/spark/dependencies/tqdm.zip has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType, StringType,StructField\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.extraClassPath\", \"/root/airflowmount/spark/connectors/mssql-jdbc-9.4.1.jre8.jar\").\\\n",
    "        config(\"spark.driver.extraClassPath\", \"/root/airflowmount/spark/connectors/mssql-jdbc-9.4.1.jre8.jar\").\\\n",
    "        config(\"spark.executor.memory\",'10g') .\\\n",
    "        getOrCreate()\n",
    "\n",
    "spark.sparkContext.addPyFile(\"/root/airflowmount/spark/dependencies/clfparser.zip\")\n",
    "spark.sparkContext.addPyFile(\"/root/airflowmount/spark/dependencies/faker.zip\")\n",
    "spark.sparkContext.addPyFile(\"/root/airflowmount/spark/dependencies/text_unidecode.zip\")\n",
    "spark.sparkContext.addPyFile(\"/root/airflowmount/spark/dependencies/six.py\")\n",
    "spark.sparkContext.addPyFile(\"/root/airflowmount/spark/dependencies/random_user_agent.zip\")\n",
    "spark.sparkContext.addPyFile(\"/root/airflowmount/spark/dependencies/tqdm.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15580e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_name = \"jdbc:sqlserver://host.docker.internal\"\n",
    "database_name = \"B2BPlatform\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "\n",
    "table_name = \"customers\"\n",
    "username = \"sa\"\n",
    "password = \"toptal\" # Please specify password here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e84ec0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/22 19:01:50 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/01/22 19:01:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:716)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:152)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:258)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:168)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df_customers = spark\\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"port\", 1533)\\\n",
    "    .option(\"dbtable\",table_name) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9c8f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184.105.1.28\n"
     ]
    }
   ],
   "source": [
    "#Fake Ip address \n",
    "from faker import Faker  \n",
    "faker = Faker()  \n",
    "ip_addr = faker.ipv4()  \n",
    "print(ip_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18cec9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/22 18:43:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Collect customers so we can iterate over them\n",
    "customers = [x[0] for x in df_customers.select(\"username\").collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab9092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def randompick(list_of):\n",
    "    max = len(list_of)\n",
    "    return customers[random.randrange(max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51f5e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-25 04:17:19\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "def random_date(start, end):\n",
    "    delta = end - start\n",
    "    int_delta = (delta.days * 24 * 60 * 60) + delta.seconds\n",
    "    random_second = randrange(int_delta)\n",
    "    return start + timedelta(seconds=random_second)\n",
    "\n",
    "d1 = datetime.strptime('1/1/2021 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "d2 = datetime.strptime('12/31/2021 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "print(random_date(d1, d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2c9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_useragents = spark.read.format('text').load('/root/airflowmount/spark/assets/user_agents.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc8f0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "useragents = [x[0] for x in df_useragents.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccbaf337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:11<00:00, 8802.70it/s]\n",
      "100%|██████████| 100000/100000 [00:11<00:00, 8890.89it/s]\n",
      "100%|██████████| 100000/100000 [00:13<00:00, 7660.45it/s]\n",
      "100%|██████████| 100000/100000 [00:11<00:00, 8476.73it/s]\n",
      "100%|██████████| 100000/100000 [00:12<00:00, 8286.47it/s]\n",
      "100%|██████████| 100000/100000 [00:12<00:00, 8080.92it/s]\n",
      " 53%|█████▎    | 52784/100000 [00:07<00:06, 7272.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m loglist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,amountOfLogs)):\n\u001b[0;32m----> 8\u001b[0m     logline \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfaker\u001b[38;5;241m.\u001b[39mipv4()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandompick(customers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_date(d1, d2)\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -0700] \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mGET /favicon.ico HTTP/1.1\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m 404 209 \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mhttp://www.example.com/start.html\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00museragents[random\u001b[38;5;241m.\u001b[39mrandrange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(useragents))]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     loglist\u001b[38;5;241m.\u001b[39mappend(Row(logline))\n\u001b[1;32m     11\u001b[0m logSchema \u001b[38;5;241m=\u001b[39m StructType([       \n\u001b[1;32m     12\u001b[0m     StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m ])\n",
      "File \u001b[0;32m/tmp/spark-33f3eaa5-367d-481f-8fd9-f2d3cd81436a/userFiles-e5420ade-2d03-40d2-a203-97ff09d91b8b/faker.zip/faker/providers/internet/__init__.py:562\u001b[0m, in \u001b[0;36mProvider.ipv4\u001b[0;34m(self, network, address_class, private)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    561\u001b[0m     all_networks, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_all_networks_and_weights(address_class\u001b[38;5;241m=\u001b[39maddress_class)\n\u001b[0;32m--> 562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_random_ipv4_address_from_subnets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_networks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/spark-33f3eaa5-367d-481f-8fd9-f2d3cd81436a/userFiles-e5420ade-2d03-40d2-a203-97ff09d91b8b/faker.zip/faker/providers/internet/__init__.py:464\u001b[0m, in \u001b[0;36mProvider._random_ipv4_address_from_subnets\u001b[0;34m(self, subnets, weights, network)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# If the weights argument has an invalid value, default to equal distribution\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subnets) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights)\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(w, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights)\n\u001b[1;32m    461\u001b[0m ):\n\u001b[1;32m    462\u001b[0m     subnet \u001b[38;5;241m=\u001b[39m choices_distribution(\n\u001b[1;32m    463\u001b[0m         subnets,\n\u001b[0;32m--> 464\u001b[0m         [\u001b[38;5;28mfloat\u001b[39m(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights],\n\u001b[1;32m    465\u001b[0m         random\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mrandom,\n\u001b[1;32m    466\u001b[0m         length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    467\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     subnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(subnets)\n",
      "File \u001b[0;32m/tmp/spark-33f3eaa5-367d-481f-8fd9-f2d3cd81436a/userFiles-e5420ade-2d03-40d2-a203-97ff09d91b8b/faker.zip/faker/providers/internet/__init__.py:464\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# If the weights argument has an invalid value, default to equal distribution\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subnets) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights)\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(w, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights)\n\u001b[1;32m    461\u001b[0m ):\n\u001b[1;32m    462\u001b[0m     subnet \u001b[38;5;241m=\u001b[39m choices_distribution(\n\u001b[1;32m    463\u001b[0m         subnets,\n\u001b[0;32m--> 464\u001b[0m         [\u001b[38;5;28mfloat\u001b[39m(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights],\n\u001b[1;32m    465\u001b[0m         random\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mrandom,\n\u001b[1;32m    466\u001b[0m         length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    467\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     subnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(subnets)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#generate logs in 100k batches\n",
    "for i in range(0,1):\n",
    "    amountOfLogs = 100000\n",
    "    loglist = []\n",
    "\n",
    "    for i in tqdm(range(0,amountOfLogs)):\n",
    "        logline = f\"{faker.ipv4()} - {randompick(customers)} [{random_date(d1, d2).strftime('%d/%b/%Y:%H:%m:%S')} -0700] \\\"GET /favicon.ico HTTP/1.1\\\" 404 209 \\\"http://www.example.com/start.html\\\" \\\"{useragents[random.randrange(0,len(useragents))]}\\\"\"\n",
    "        loglist.append(Row(logline))\n",
    "\n",
    "    logSchema = StructType([       \n",
    "        StructField('log', StringType(), True)\n",
    "    ])\n",
    "\n",
    "    df_logs = spark.createDataFrame(data=loglist, schema=logSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs.write.format('text').mode('overwrite').save('/root/airflowmount/spark/storage/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e703c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
